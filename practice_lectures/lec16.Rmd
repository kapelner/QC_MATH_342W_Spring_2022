---
title: "Practice Lecture 16 MATH 342W Queens College"
author: "Professor Adam Kapelner"
date: "April 5, 2021"
---


## Reducing variance with Cross Validation (i.e. K-fold CV)

We saw previous there was a lot of variance in generalization error estimation. We can reduce some of this variance by using a very simple trick. We can rotate the train-test split so that each observation will be in the test set once. How many times is this done? K. Now we see the reason for the definition of K as it tells you how many times you validate. Why is it called "cross"? Because the training set crosses over as it does the rotation. Each observation is inside a training set K-1 times. This point will become important later. Why is it called K-fold? Because a fold is one set of training-test and there are K unique folds during the whole procedure.

How does this work? Well, let's say K=10, a typical value. This means in each "fold", 90% of the data is in the training set and 10% of the data is in the test set. As we run through the K folds, we train a model on the training set and predict on the test set and compute oos residuals We aggregate those oos residuals over the folds to result in n oos residuals. We then run our error metric on all n.

Let's begin with the dataset from the previous demo. Here is that code that will create the folds by specifying the K=10 test sets by index. The training sets can then be found by the set difference function.

```{r}
K = 10
set.seed(1984)
temp = rnorm(n)
observation_folds = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)
table(observation_folds)
```

We now do our first cross validation of the linear model.

```{r}
oos_cv_residuals = array(NA, n)
for (k in 1 : K){
  index_test = which(observation_folds == k)
  index_train = setdiff(1 : n, index_test)
  
  x_train = x[index_train]
  y_train = y[index_train]
  Xtrain = data.frame(x = x_train, y = y_train)
  x_test = x[index_test]
  y_test = y[index_test]
  
  g_mod = lm(y ~ ., Xtrain)
  y_hat_g = predict(g_mod, data.frame(x = x_test))
  oos_cv_residuals[index_test] = y_test - y_hat_g
}

sd(oos_cv_residuals)
```

How does this CV error look over K? 

```{r}
Kuniqs = setdiff(divisors(n), 1)
results = data.frame(K = Kuniqs, s_e = NA)

set.seed(1984)
for (K in Kuniqs){
    temp = rnorm(n)
    observation_folds = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)
  oos_residuals = array(NA, n)
  for (k in 1 : K){
    index_test = which(observation_folds == k)
    index_train = setdiff(1 : n, index_test)
    
    x_train = x[index_train]
    y_train = y[index_train]
    Xtrain = data.frame(x = x_train, y = y_train)
    x_test = x[index_test]
    y_test = y[index_test]
    
    g_mod = lm(y ~ ., Xtrain)
    y_hat_g = predict(g_mod, data.frame(x = x_test))
    oos_residuals[index_test] = y_test - y_hat_g
  }
  
  results[results$K == K, "s_e"] = sd(oos_residuals)
}
results
```

There is still an effect of the one random fold. Let's do this many times and look at the distribution just like before.

```{r}
Nsim_per_K = 500
num_Kuniqs = length(Kuniqs)
Ks = rep(Kuniqs, Nsim_per_K)
results = data.frame(s_e = rep(NA, Nsim_per_K * num_Kuniqs), K = rep(NA, Nsim_per_K * num_Kuniqs))


set.seed(1984)
for (i in 1 : length(Ks)){
  K = Ks[i]
  temp = rnorm(n) #this makes it a different fold each time
  observation_folds = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)
  oos_residuals = array(NA, n)
  for (k in 1 : K){
    index_test = which(observation_folds == k)
    index_train = setdiff(1 : n, index_test)
    
    x_train = x[index_train]
    y_train = y[index_train]
    Xytrain = data.frame(x = x_train, y = y_train)
    x_test = x[index_test]
    y_test = y[index_test]
    
    g_mod = lm(y ~ ., Xytrain)
    y_hat_g = predict(g_mod, data.frame(x = x_test))
    oos_residuals[index_test] = y_test - y_hat_g
  }
  results[i, ] = c(sd(oos_residuals), K)
}
```

What is the variability?

```{r}
results_summary = results %>%
  group_by(K) %>%
  summarize(Kavg = mean(s_e), Kse = sd(s_e))
results_summary
```

This is a significant improvement in variability than before! (scroll up) There is greatly improved tightness for high K. Seemingly with K-fold CV, you can be more confident to use high K because it is decreasing the variance in the estimate. High K also reduces bias.

Now we plot it:

```{r}
ggplot(results) + 
  aes(x = s_e) +
  geom_density(aes(fill = factor(K)), alpha = 0.3) + 
  # xlim(0, NA) +
  xlim(3, 4.5) +
  geom_vline(data = results_summary, aes(xintercept = Kavg, color = factor(K)), size = 2) +
  geom_vline(xintercept = gen_error_true, col = "white", size = 1)
```


Admittedly, I don't know the properties of CV estimates as well as I should. Thus, there will be only procedural questions on the next exam. I do know that selecting K "optimally" for general datasets is an open question.

There is one other nice thing about having folds, you can estimate the standard error in your generalization estimate by pretending you have K iid samples and pretending the normal theory applies. For example, let's say K = 5. Instead of aggregating all residuals, we leave them separate and get K = 5 difference estimates for generalization error.

```{r}
K = 5
set.seed(1984)
temp = rnorm(n)
observation_folds = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)

oos_s_e_s = array(NA, K)
for (k in 1 : K){
  index_test = which(observation_folds == k)
  index_train = setdiff(1 : n, index_test)
  
  x_train = x[index_train]
  y_train = y[index_train]
  Dtrain = data.frame(x = x_train, y = y_train)
  x_test = x[index_test]
  y_test = y[index_test]
  
  g_mod = lm(y ~ ., Dtrain)
  y_hat_g = predict(g_mod, data.frame(x = x_test))
  oos_s_e_s[k] = sd(y_test - y_hat_g)
}

avg_s_e = mean(oos_s_e_s)
s_s_e = sd(oos_s_e_s)
avg_s_e
s_s_e
#approx 95% CI
c(avg_s_e - 2 * s_s_e, avg_s_e + 2 * s_s_e) #no divide by sqrt(K) - that was a mistake in the notes
gen_error_true
```

Although this is technically nonsense since they're not iid samples since the training set is crossed over containing mostly the same observations, at least it's something. In the above example, we've managed to capture the true generalization error.

Coverage in this confidence interval is over D. So I wouldn't gain much insight by simulating different splits with the same K.

I believe confidence intervals for generalization error is an open problem or maybe proved that you can't find them in general situations.

Here's a real data example with the `diamonds` dataset.

```{r}
K = 5

set.seed(2000)
temp = rnorm(nrow(diamonds))
observation_folds = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)

all_idx = 1 : nrow(diamonds)
s_e_s = array(NA, K)
y_hat_g = array(NA, nrow(diamonds))
for (k in 1 : K){
  index_test = which(observation_folds == k)
  index_train = setdiff(1 :  nrow(diamonds), index_test)
  mod = lm(price ~ ., diamonds[index_train, ])
  y_hat_g[index_test] = predict(mod, diamonds[index_test, ])
  s_e_s[k] = sd(diamonds[index_test, ]$price - y_hat_g[index_test])
}
s_e_s
mean(s_e_s)
sd(s_e_s)
#approx 95% CI
c(mean(s_e_s) - 2 * sd(s_e_s), mean(s_e_s) + 2 * sd(s_e_s))
```

Why is the $s_{s_e}$ so low? High $n$. Cross validation here was probably not even necessary.

