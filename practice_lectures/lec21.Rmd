---
title: "Practice Lecture 21 MATH 342W Queens College"
author: "Professor Adam Kapelner"
date: "April 21, 2021"
---



# Using Probability Estimation to do Classification

First repeat quickly (a) load the adult data (b) do a training / test split and (c) build the logisitc model.

```{r}
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness

set.seed(1)
train_size = 5000
train_indices = sample(1 : nrow(adult), train_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL

test_size = 5000
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL

logistic_mod = glm(income ~ ., adult_train, family = "binomial")
p_hats_train = predict(logistic_mod, adult_train, type = "response")
p_hats_test = predict(logistic_mod, adult_test, type = "response")
```

Let's establish a rule: if the probability estimate is greater than or equal to 50%, let's classify the observation as positive, otherwise 0.

```{r}
y_hats_train = factor(ifelse(p_hats_train >= 0.5, ">50K", "<=50K"))
```

How did this "classifier" do in-sample?

```{r}
mean(y_hats_train != y_train)
table(y_train, y_hats_train)
```

Let's see the same thing oos:

```{r}
y_hats_test = factor(ifelse(p_hats_test >= 0.5, ">50K", "<=50K"))
mean(y_hats_test != y_test)
oos_conf_table = table(y_test, y_hats_test)
oos_conf_table
```

A tad bit worse. Here are estimates of the future performance for each class:

```{r}
n = sum(oos_conf_table)
fp = oos_conf_table[1, 2]
fn = oos_conf_table[2, 1]
tp = oos_conf_table[2, 2]
tn = oos_conf_table[1, 1]
num_pred_pos = sum(oos_conf_table[, 2])
num_pred_neg = sum(oos_conf_table[, 1])
num_pos = sum(oos_conf_table[2, ])
num_neg = sum(oos_conf_table[1, ])
precision = tp / num_pred_pos
cat("precision", round(precision * 100, 2), "%\n")
recall = tp / num_pos
cat("recall", round(recall * 100, 2), "%\n")
false_discovery_rate = 1 - precision
cat("false_discovery_rate", round(false_discovery_rate * 100, 2), "%\n")
false_omission_rate = fn / num_pred_neg
cat("false_omission_rate", round(false_omission_rate * 100, 2), "%\n")
```

This whole classifier hinged on the decision of 50%! What if we change it??

# Asymmetric Cost Classifiers

Let's establish a *new* rule: if the probability estimate is greater than or equal to 90%, let's classify the observation as positive, otherwise 0.

```{r}
y_hats_test = factor(ifelse(p_hats_test >= 0.9, ">50K", "<=50K"))
mean(y_hats_test != y_test)
oos_conf_table = table(y_test, y_hats_test)
oos_conf_table
```

Of course the misclassification error went up! But now look at the confusion table! The second column represents all $\hat{y} = 1$ and there's not too many of them! Why? You've made it *much* harder to classify something as positive. Here's the new additional performance metrics now:

```{r}
n = sum(oos_conf_table)
fp = oos_conf_table[1, 2]
fn = oos_conf_table[2, 1]
tp = oos_conf_table[2, 2]
tn = oos_conf_table[1, 1]
num_pred_pos = sum(oos_conf_table[, 2])
num_pred_neg = sum(oos_conf_table[, 1])
num_pos = sum(oos_conf_table[2, ])
num_neg = sum(oos_conf_table[1, ])
precision = tp / num_pred_pos
cat("precision", round(precision * 100, 2), "%\n")
recall = tp / num_pos
cat("recall", round(recall * 100, 2), "%\n")
false_discovery_rate = 1 - precision
cat("false_discovery_rate", round(false_discovery_rate * 100, 2), "%\n")
false_omission_rate = fn / num_pred_neg
cat("false_omission_rate", round(false_omission_rate * 100, 2), "%\n")
```

We don't make many false discoveries but we make a lot of false omissions! It's a tradeoff...
