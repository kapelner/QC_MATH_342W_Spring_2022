---
title: "Practice Lectures Week 10 MATH 342W Queens College"
author: "Professor Adam Kapelner"
date: "Apr 11, 2022"
---


# Regression Trees

Let's fit a regression tree. We will use the development package `YARF` which I've been hacking on now for a few years. The package internals are written in Java which we just installed above. Since `YARF` is not on CRAN, we install the package from my github including its dependency (if necessary) and then load it 

```{r}
if (!pacman::p_isinstalled(YARF)){
  pacman::p_install_gh("kapelner/YARF/YARFJARs", ref = "dev")
  pacman::p_install_gh("kapelner/YARF/YARF", ref = "dev", force = TRUE)
}
options(java.parameters = "-Xmx4000m")
pacman::p_load(YARF)
```

The data will be fitting with the regression tree is a sine curve plus noise:

```{r}
pacman::p_load(tidyverse, magrittr)
n = 500
x_max = 4 * pi
x = runif(n, 0, x_max)
sigma = 0.05
y = ifelse(x < 2 * pi, 0.5 * sin(x), 0) + rnorm(n, 0, sigma)
ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point(lwd = 0.6) 
```

Now we fit a regression tree to this model. Nevermind the `calculate_oob_error` argument for now. This will be clear why FALSE is NOT the default in a few classes.

```{r}
tree_mod = YARFCART(data.frame(x = x), y, calculate_oob_error = FALSE)
```

How "big" is this tree model?

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

What are the "main" splits?

```{r}
illustrate_trees(tree_mod, max_depth = 4, open_file = TRUE)
```

What does $g(x)$ look like?

```{r}
Nres = 1000
x_predict = data.frame(x = seq(0, x_max, length.out = Nres))
g = predict(tree_mod, x_predict)
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x_predict, y = g), col = "blue")
```

Obviously overfit - but not that bad... let's try lowering the complexity by stopping the tree construction at a higher node size.

```{r}
tree_mod = YARFCART(data.frame(x = x), y, nodesize = 50, calculate_oob_error = FALSE)
yhat = predict(tree_mod, x_predict)
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x_predict, y = yhat), col = "blue")
```

Less overfitting now but now it's clearly underfit! We can play with the nodesize. Or we can use the model selection algorithm to pick the model (the nodesize). Let's ensure nodesize = 1 gives us perfect overfitting.

```{r}
tree_mod = YARFCART(data.frame(x = x), y, nodesize = 1, calculate_oob_error = FALSE)
yhat = predict(tree_mod, data.frame(x = x))
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x, y = yhat), col = "blue")
mean((y - yhat)^2)
```
Are we sure we have a leaf node for each observation?

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

Let's ensure nodesize > n gives us perfect underfitting.

```{r}
tree_mod = YARFCART(data.frame(x = x), y, nodesize = n + 1, calculate_oob_error = FALSE)
yhat = predict(tree_mod, data.frame(x = x))
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x, y = yhat), col = "blue")
unique(yhat)
mean(y)
```

Are we sure we have one root node?

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

Yes.


Let's try this using oos validation and trace out a performance curve to find the optimal hyperparameter.

```{r}
K = 4
set.seed(1984)
select_idx = sample(1 : n, round(1 / K * n))
x_select = x[select_idx]
y_select = y[select_idx]
train_idx = setdiff(1 : n, select_idx)
x_train = x[train_idx]
y_train = y[train_idx]
n_train = length(train_idx)

max_nodesize_to_try = 70
in_sample_errors = array(NA, max_nodesize_to_try)
oos_errors = array(NA, max_nodesize_to_try)
for (i in 1 : max_nodesize_to_try){
  tree_mod = YARFCART(data.frame(x = x_train), y_train, nodesize = i, calculate_oob_error = FALSE)
  yhat = predict(tree_mod, data.frame(x = x_train))
  in_sample_errors[i] = sd(y_train - yhat)
  yhat_select = predict(tree_mod, data.frame(x = x_select))
  oos_errors[i] = sd(y_select - yhat_select)
}

ggplot(data.frame(nodesize = 1: max_nodesize_to_try, in_sample_errors = in_sample_errors, oos_errors = oos_errors)) + 
  geom_point(aes(nodesize, in_sample_errors), col = "red") +
  geom_point(aes(nodesize, oos_errors), col = "blue")
```

For some reason, we do not see serious overfitting.

# Regression Trees with Real Data

Now let's look at a regression tree model predicting medv in the Boston Housing data. We first load the data and do a training-test split:

```{r}
set.seed(1984)
pacman::p_load(MASS)
data(Boston)
test_prop = 0.1
train_indices = sample(1 : nrow(Boston), round((1 - test_prop) * nrow(Boston)))
Boston_train = Boston[train_indices, ]
y_train = Boston_train$medv
X_train = Boston_train
X_train$medv = NULL
n_train = nrow(X_train)
```

And fit a tree model. The default hyperparameter, the node size is $N_0 = 5$.

```{r}
options(java.parameters = "-Xmx4000m")
pacman::p_load(YARF)
tree_mod = YARFCART(X_train, y_train, calculate_oob_error = FALSE)
```

What does the in-sample fit look like?

```{r}
y_hat_train = predict(tree_mod, X_train)
e = y_train - y_hat_train
sd(e)
1 - sd(e) / sd(y_train)
```

Recall the linear model:

```{r}
linear_mod = lm(medv ~ ., Boston_train)
sd(y_train - linear_mod$fitted.values)
summary(linear_mod)$r.squared
```

The tree seems to win in-sample. Why? 

Is this a "fair" comparison?

Before we address this, let's illustrate the tree. 

```{r}
illustrate_trees(tree_mod, max_depth = 4, open_file = TRUE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

Let's make the comparison fair by seeing what happens oos.

```{r}
test_indices = setdiff(1 : nrow(Boston), train_indices)
Boston_test = Boston[test_indices, ]
y_test = Boston_test$medv
X_test = Boston_test
X_test$medv = NULL
```

For the tree:

```{r}
y_hat_test_tree = predict(tree_mod, X_test)
e = y_test - y_hat_test_tree
sd(e)
1 - sd(e) / sd(y_test)
```

For the linear model:

```{r}
y_hat_test_linear = predict(linear_mod, Boston_test)
e = y_test - y_hat_test_linear
sd(e)
1 - sd(e) / sd(y_test)
```

The take-home message here is that the tree beats the linear model in future predictive performance but the only way to be truly convinced of this is to do the split over and over to get a sense of the average over the massive variability (like the previous demo) or to do CV to reduce the error of the estimate. 

Why does the regression tree beat the linear model? Let's see what's going on in the tree.

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

About how many observations are in each leaf?

```{r}
nrow(Boston_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

That's a very flexible model.

Let's see overfitting in action. Let's set nodesize to be one.

```{r}
tree_mod = YARFCART(X_train, y_train, nodesize = 1, calculate_oob_error = FALSE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(Boston_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

Why is it not exactly 1 on average? I think it's because...

```{r}
data.table::uniqueN(y_train)
length(y_train)
```

Regardless of this point, this model is essentially giving each observation it's own y-hat, it's own personal guess which will be its own personal y. Just like linear modeling when $n = p + 1$ and nearest neighbors when $K = 1$. Let's see how bad the overfitting is:

```{r}
y_hat_train = predict(tree_mod, X_train)
e = y_train - y_hat_train
sd(e)
1 - sd(e) / sd(y_train)
```

This is the expected behavior in perfect fitting.

```{r}
y_hat_test_tree = predict(tree_mod, X_test)
e = y_test - y_hat_test_tree
sd(e)
1 - sd(e) / sd(y_test)
```

It overfits but amazing it doesn't get clobbered completely! And its results are on-par with the non-overfit linear model probably because it made up for the overfitting by reducing misspecification error. 

Trees are truly amazing!


# Classification Trees

Let's get the cancer biopsy data:

```{r}
rm(list = ls())
pacman::p_load(YARF, tidyverse, magrittr)
data(biopsy, package = "MASS")
biopsy %<>% na.omit %>% dplyr::select(-ID) #for some reason the "select" function is scoping elsewhere without this explicit directive
colnames(biopsy) = c(
  "clump_thickness",
  "cell_size_uniformity",
  "cell_shape_uniformity",
  "marginal_adhesion",
  "epithelial_cell_size",
  "bare_nuclei",
  "bland_chromatin",
  "normal_nucleoli",
  "mitoses",
  "class"
)
```

Let's do a training-test split to keep things honest:

```{r}
test_prop = 0.1
train_indices = sample(1 : nrow(biopsy), round((1 - test_prop) * nrow(biopsy)))
biopsy_train = biopsy[train_indices, ]
y_train = biopsy_train$class
X_train = biopsy_train
X_train$class = NULL
n_train = nrow(X_train)
test_indices = setdiff(1 : nrow(biopsy), train_indices)
biopsy_test = biopsy[test_indices, ]
y_test = biopsy_test$class
X_test = biopsy_test
X_test$class = NULL
```

Let's fit a tree:

```{r}
tree_mod = YARFCART(X_train, y_train, calculate_oob_error = FALSE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(biopsy_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

Why would the average observations per node be larger than the nodesize which is 1?

```{r}
illustrate_trees(tree_mod, max_depth = 5, length_in_px_per_half_split = 30, open_file = TRUE)
```

How are we doing in-sample?

```{r}
y_hat_train = predict(tree_mod, X_train)
mean(y_train != y_hat_train)
```

Why is this?

```{r}
?YARFCART
```

That's the default for classification tree algorithm. Don't be lazy: you probably should use a Dselect and optimize nodesize!!!

Out of sample?

```{r}
y_hat_test = predict(tree_mod, X_test)
mean(y_test != y_hat_test)
```

It appears we overfit. But this is still pretty good!

Now let's take a look at the linear SVM model. Let's use default cost (we should really CV over the cost parameter if we weren't lazy).

```{r}
pacman::p_load(e1071)
svm_model = svm(X_train, y_train, kernel = "linear")
svm_model
```

A couple of points:

* Reached max iterations to minimize the hinge loss. Seems like there are computational issues here.
* Note that we are relying on the $\lambda$ hyperparameter value for the hinge loss. On the homework, you will answer the question we never answered: how should the value of the hyperparameter be chosen?

Regardless, how did it do in-sample?

```{r}
y_hat_train = predict(svm_model, X_train)
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(svm_model, X_test)
mean(y_test != y_hat_test)
```

Maybe the model truly was linearly separable? Meaning, you don't get any added benefit from the tree if there are no interactions or non-linearities. Let's try a harder dataset. First, get a bunch of datasets from the UCI repository:

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness
?adult
```

Let's use samples of 2,000 to run experiments:

```{r}
set.seed(1984)
test_size = 2000
train_indices = sample(1 : nrow(adult), test_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL
n_train = nrow(X_train)
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Make a tree:

```{r}
tree_mod = YARFCART(X_train, y_train, calculate_oob_error = FALSE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(adult_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
illustrate_trees(tree_mod, max_depth = 5, length_in_px_per_half_split = 30, open_file = TRUE)
```

In-sample?

```{r}
y_hat_train = predict(tree_mod, X_train)
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(tree_mod, X_test)
mean(y_test != y_hat_test)
```

The warning was legit this time. What's it saying?

Looks like we overfit quite a bit! That's what nodesize of 1 does! Why is it the default? People found that even though it overfits, you still get good performance (as we've seen even with regression). I doubt people still use CART in production models that require best possible accuracy these days since this issue of overfitting was fixed with bagging (we will get to this soon) and Random Forests (we'll get to that too).

Let's see how the linear SVM does. Warning: this takes a while to compute:

```{r}
svm_model = svm(model.matrix(~ ., X_train), y_train, kernel = "linear")
```

In-sample?

```{r}
y_hat_train = predict(svm_model, model.matrix(~ ., X_train))
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(svm_model, model.matrix(~ ., X_test))
mean(y_test != y_hat_test)
```

It seems (at least when I ran it at home), the linear SVM does much worse. Likely there are a lot of interactions in this dataset that the linear SVM must ignore because it's $\mathcal{H}$ candidate set is so limited!

Note: SVM train error is approximtely = SVM test error? Why? 

That's a usual scenario during underfitting in high n situations. There is no estimation error - only misspecification error and error due to ignorance. And those are the same among the training and test set.



