---
title: "Practice Lecture 18 MATH 342W Queens College"
author: "Professor Adam Kapelner"
date: "April 12, 2021"
---


## Data "Munging" with Dplyr and data.table

"Data munging", sometimes referred to as "data wrangling", is the process of transforming and mapping data from one "raw" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. A data wrangler is a person who performs these transformation operations. -[Wikipedia](https://en.wikipedia.org/wiki/Data_wrangling).

Half of what a data scientist does is cleaning data, visualizing data and wrangling it. In the process you learn all about your dataset and you're on a higher level when it comes time to build prediction models.

The packages `dplyr` and `data.table` offer many conveninent functions to manipulate, clean, and otherwise wrangle data. Note: all the wrangling we're going to see *can* be done with base R (see previous notes on the `data.frame` object) but it would be *very very very very* annoying and *very very very very* slow.

I will quickly compare and contrast `dplyr` and `data.table` before you see it inside actual code.

* `dplyr` works really nicely with the piping chain as you "begin" the manipulation with the dataset and then iteratively pipe in step 1, step 2, etc until you wind up with what end product you would like. This makes `dplyr` very readable but very verbose - lots of lines of code. 
* On the flip side, `data.table` essentially wrote a new data wrangling language so it's a harder learning curve but it's very compact - very few lines of code.
* `data.table` is blazing fast and kills `dplyr` in performance and I'm pretty sure it even beats Python in performance (someone please check this). So in the era of "big data", I think this is the winner even though it is much harder to learn.
* I believe `dplyr` is more popular in the real world and thus has more cache to put on your CV. But this is constantly in flux!

For all labs and the final project, you are recommended to pick one you want to use and go with it. For the exams, I will write code in both (if need be) to not penalize / reward a student who picked one over the other.

Here is a nice [translation guide](https://atrebas.github.io/post/2019-03-03-datatable-dplyr/) between `dplyr` and `data.table`. We will be learning them in tandem. I could've split this into two units but I decided against it because (1) it is good to see the same functionality side-by-side and (2) this is really just one concept.

```{r}
pacman::p_load(tidyverse, magrittr) #tidyverse is shorthard for dplyr, ggplot2, tidyr, readr and a bunch of other packages recommended for the "full" dplyr experience. I'm using magrittr for special pipe operations later.
pacman::p_load(data.table)
```

First, recall what pipe format means! We're going to need to know this well for what's coming next...

```{r}
set.seed(1984)
mean(head(round(sample(rnorm(1000), 100), digits = 2)))

set.seed(1984)
rnorm(1000) %>% #the pipe operator
  sample(100) %>% 
  round(digits = 2) %>% #the first argument is passed in automatically.
  head %>%
  mean
```

Note that `data.table` is automatically multithreaded. Read [here](https://www.rdocumentation.org/packages/data.table/versions/1.12.8/topics/setDTthreads).

```{r}
getDTthreads()
```


We first instantiate the upgraded data.frame objects in both libraries:

```{r}
diamonds_tbl = tbl_df(diamonds) #not necessary to cast because dplyr does the conversion automatically after using any dplyr function
diamonds_dt = data.table(diamonds) #absolutely necessary
```

What happens during the data frame conversion?

```{r}
class(diamonds_tbl)
class(diamonds_dt)
```

Note how these are implemented as class extensions of R's `data.frame` as to allow for background compatibility and not break the API. They have nicer ways of showing the data:

```{r}
diamonds_tbl #run this in the console, not inside the chunk
diamonds_dt #run this in the console, not inside the chunk
```

Beginning with the simplest munging tasks, subsetting rows:

```{r}
diamonds_tbl %>% 
  slice(1 : 5)

diamonds_dt[1 : 5]
```

And subsetting columns:

```{r}
diamonds_tbl %>% 
  select(cut, carat, price) #these three only in this order

diamonds_dt[, .(cut, carat, price)]

diamonds_tbl %>% 
  select(carat, price, cut) #these three only in another order

diamonds_dt[, .(carat, price, cut)]

diamonds_tbl %>% 
  select(-x) #drop this feature

diamonds_dt[, !"x"]
#diamonds_dt[, x := NULL] #mutating function (overwrites the data frame)

diamonds_tbl %>% 
  select(-c(x, y, z)) #drop these features
diamonds_tbl %>% 
  select(-x, -y, -z) #drop these features

diamonds_dt[, !c("x", "y", "z")]
```

How about will rename a column

```{r}
diamonds_tbl %>% 
  rename(weight = carat, price_USD = price)

diamonds_dt_copy = copy(diamonds_dt)
setnames(diamonds_dt_copy, old = c("carat", "price"), new = c("weight", "price_USD")) #the `setnames` function is mutating, i.e. it modifies the data.table object, so I made a copy as to not alter the table for the rest of the demo
diamonds_dt_copy
rm(diamonds_dt_copy)
```

If you want to rearrange the columns...

```{r}
#In dplyr you pretend to select a subset and then ask for everything else:
diamonds_tbl %>% 
  select(carat, price, cut, everything()) #these three in this order first then everything else
# diamonds_tbl %>% 
#   select(-carat, everything()) #move carat last (first drop it, and then add it back in with everything)

diamonds_dt_copy = copy(diamonds_dt)
setcolorder(diamonds_dt_copy, c("carat", "price", "cut")) #as before, the `setcolorder` function is mutating, i.e. it modifies the data.table object, so I made a copy as to not alter the table for the rest of the demo
diamonds_dt_copy
rm(diamonds_dt_copy)
```

Sorting the rows by column(s):

```{r}
diamonds_tbl %>%
  arrange(carat) #default is ascending i.e. lowest first

diamonds_dt[order(carat)]
diamonds_dt_copy = copy(diamonds_dt)
setorder(diamonds_dt_copy, carat) #as before, the `setorder` function is mutating, i.e. it modifies the data.table object, so I made a copy as to not alter the table for the rest of the demo
diamonds_dt_copy
rm(diamonds_dt_copy)

diamonds_tbl %>%
  arrange(desc(carat)) #switch to descending, i.e. highest first

diamonds_dt[order(-carat)] #and you can do this with `setorder` too

diamonds_tbl %>%
  arrange(desc(color), clarity, cut, desc(carat)) #multiple sorts - very powerful

diamonds_dt[order(-color, clarity, cut, -carat)] #and you can do this with `setorder` too
```

The filter method subsets the data based on conditions:

```{r}
diamonds_tbl %>%
  filter(cut == "Ideal")

diamonds_dt[cut == "Ideal"]

diamonds_tbl %>%
  filter(cut == "Ideal") %>%
  filter(depth < 65) %>%
  filter(x * y * z > 20)
diamonds_tbl %>%
  filter(cut == "Ideal" & depth < 65 & x * y * z > 20)

diamonds_dt[cut == "Ideal" & depth < 65 & x * y * z > 20]

diamonds_tbl %>%
  filter((cut == "Ideal" | cut == "Premium") & depth < 65 & x * y * z > 20)

diamonds_dt[(cut == "Ideal" | cut == "Premium") & depth < 65 & x * y * z > 20]

diamonds_tbl %>%
  filter(cut %in% c("Ideal", "Premium") & depth < 65 & x * y * z > 20)

diamonds_dt[cut %in% c("Ideal", "Premium") & depth < 65 & x * y * z > 20]
```

How about removing all rows that are the same?

```{r}
diamonds_tbl
diamonds_tbl %>%
  distinct

unique(diamonds_dt)

#nice function from data.table:
uniqueN(diamonds$carat) 
#273 < 53940 i.e. there's only a few weight measurements that are possible... let's only keep one from each unique carat value

diamonds_tbl %>%
  distinct(carat, .keep_all = TRUE) #keeps the first row for each unique weight measurement

unique(diamonds_dt, by = "carat")
```

Sampling is easy

```{r}
diamonds_tbl %>%
  sample_n(7)

diamonds_dt[sample(.N, 7)] #.N is a cool function: it is short for `nrow(dt object)`

diamonds_tbl %>%
  sample_frac(1e-3)

diamonds_dt[sample(.N, .N * 1e-3)] #.N is a cool function: it is short for `nrow(dt object)
```


Now for some real fun stuff. Let's create new features with the `mutate` function.

```{r}
diamonds_tbl %>%
  mutate(volume = x * y * z) #adds a new column keeping the old ones (this was an exam problem in a previous year)

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, volume := x * y * z]
diamonds_dt2

diamonds_tbl %>%
  mutate(price_per_carat = price / carat) %>%
  arrange(desc(price_per_carat))

diamonds_dt2[, price_per_carat := price / carat]
diamonds_dt2[order(-price_per_carat)]
rm(diamonds_dt2)
```

Or rewrite old ones.

```{r}
diamonds_tbl %>%
  mutate(cut = substr(cut, 1, 1))

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, cut := substr(cut, 1, 1)]
diamonds_dt2

diamonds_tbl %>%
  mutate(carat = factor(carat))

diamonds_dt2[, carat := factor(carat)]
diamonds_dt2
rm(diamonds_dt2)
```

Here are some more ways to create new variables. Translating to `data.table` is trivial:

```{r}
diamonds_tbl %>%
  mutate(carat = factor(ntile(carat, 5)))
diamonds_tbl %>%
  mutate(carat = percent_rank(carat))
diamonds_tbl %>%
  mutate(lag_price = lag(price)) #if this data was a time series
diamonds_tbl %>%
  mutate(cumul_price = cumsum(price)) #%>% tail
```

How about if you want to create a column and drop all other columns in the process?

```{r}
diamonds_tbl %>%
  transmute(volume = x * y * z) #adds a new column dropping the old ones

diamonds_dt[, .(volume = x * y * z)]
```

There are many ways to reshape a dataset. We will see two now and a few functions later when it becomes important. For instance: we can collapse columns together using the `unite` function from package `tidyr` (which should be loaded when you load `dplyr`). We will have a short unit on more exciting and useful reshapings later ("long" to "short" and vice-versa). As far as I know `data.table` has a less elegant... unless someone has a better idea?

```{r}
diamonds_tbl2 = diamonds_tbl %>%
  unite(dimensions, x, y, z, sep = " x ")
diamonds_tbl2

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, dimensions := paste(x, y, z, sep = " x ")] #mutating
diamonds_dt2 = diamonds_dt2[, !c("x", "y", "z")]
diamonds_dt2
```

We can reverse this operation:

```{r}
diamonds_tbl2 %>%
  separate(dimensions, c("x", "y", "z"), sep = " x ")
rm(diamonds_tbl2)

diamonds_dt2[, c("x", "y", "z") := strsplit(dimensions, "x")]
diamonds_dt2[, -"dimensions"]
rm(diamonds_dt2)
```

There are tons of other packages to do clever things. For instance, here's one that does dummies. Let's convert the color feature to dummies. Again slightly less readable or elegant in `data.table`:

```{r}
pacman::p_load(sjmisc, snakecase)
diamonds_tbl %>%
  to_dummy(color, suffix = "label") %>% #this creates all the dummies
  bind_cols(diamonds_tbl) %>% #now we have to add all the original data back in
  select(-matches("_"), everything()) %>% #this puts the dummies last
  select(-color) #finally we can drop color

cbind(
  diamonds_dt[, -"color"], 
  to_dummy(diamonds_dt[, .(color)], suffix = "label")
)
```


What if you want to create a new variable based on functions only run on subsets of the data. This is called "grouping". Grouping only makes sense for categorical variables. (If you group on a continuous variable, then chances are you'll have $n$ different groups because you'll have $n$ unique values).

For instance:

```{r}
diamonds_tbl %>%
  group_by(color)

diamonds_dt[,, by = color]
```

Nothing happened... these were directives to do things a bit differently with the addition of other logic. So after you group, you can now run operations on each group like they're their own sub-data frame. Usually, you want to *summarize* data by group. This means you take the entire sub-data frame and run one metric on it and return only those metrics (i.e. shrink $n$ rows to $L$ rows). This sounds more complicated than it is and it is where data wrangling really gets fun. 

Here are a few examples:

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  summarize(avg_price = mean(price))

diamonds_dt[, .(avg_price = mean(price)), by = color][order(color)] #chaining / piping [...][...][...] etc
#where did all the other rows and columns go???

diamonds_tbl %>%
  group_by(color) %>%
  summarize(avg_price = mean(price), sd_price = sd(price), count = n())

diamonds_dt[, .(avg_price = mean(price), sd_price = sd(price), count = .N), by = color][order(color)]

diamonds_tbl %>%
  group_by(color) %>%
  summarize(min_price = min(price), med_price = median(price), max_price = max(price))

diamonds_dt[, .(min_price = min(price), med_price = median(price), max_price = max(price)), by = color][order(color)]
```

Sometimes you want to do fancier things like actually run operations on the whole sub-data frame using `mutate`. If the function is a single metric, then that metric is then duplicated across the whole sub data frame.

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  mutate(avg_price_for_color = mean(price))
#creates a new feature based on running the feature only within group

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, avg_price_for_color := mean(price), by = color]
diamonds_dt2
rm(diamonds_dt2)
```

So that's kind of like duplicating a summary stat. Here's something more fun: actually creating a new vector:

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  mutate(price_rank_within_color = dense_rank(price)) #creates a new feature based on running the feature only within group

diamonds_dt2 = copy(diamonds_dt)
diamonds_dt2[, price_rank_within_color := frankv(price, ties.method = "dense"), by = color]
diamonds_dt2
rm(diamonds_dt2)
```

What if we want to get the first row in each category?

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  slice(1)

diamonds_dt[, .SD[1], by = color][order(color)]
```

The `.SD` variable is short for "sub dataframe" and it's a stand-in for the pieces of the dataframe for each color as it loops over the colors. So `.SD[1]` will be first row in the sub dataframe. The reason why the matrices come out different is that the order of the rows in data.table changes based on optimizations. We'll see some of this later. I'm also unsure why it moved the `color` column to the front.

What about first and last?

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  slice(1, n())

diamonds_dt[, .SD[c(1, .N)], by = color]
```

How about the diamond with the highest price by color?

```{r}
diamonds_tbl %>%
  group_by(color) %>%
  arrange(price) %>%
  slice(n())

diamonds_dt[, .SD[which.max(price)], by = color]
```

We've seen `data.table`'s preference for mutating functions. Here is a pipe command from package `magrittr` that makes the functions mutating. 

```{r}
diamonds_tbl2 = diamonds_tbl
diamonds_tbl2 = diamonds_tbl2 %>%
  select(-x, -y, -z) %>%
  filter(carat < 0.5) %>%
  arrange(carat, cut, color)
diamonds_tbl2

diamonds_tbl2 = diamonds_tbl
diamonds_tbl2 %<>% #pipe and overwrite (short for what's above)
  select(-x, -y, -z) %>%
  filter(carat < 0.5) %>%
  arrange(carat, cut, color)
diamonds_tbl2
rm(diamonds_tbl2)
```

This is as far we will go with data wrangling right now.

Let's benchmark a few core features of both packages. To do so, let's create a dataframe that's very big:

```{r}
pacman::p_load(microbenchmark)

Nbig = 2e6
diamonds_tbl_big = diamonds_tbl %>%
  sample_n(Nbig, replace = TRUE)
diamonds_dt_big = data.table(diamonds_tbl_big) #just to make sure we have the same data
diamonds_big = data.frame(diamonds_tbl_big) #ensure that it is a base R object
```

How about we write this dataframe to the hard drive as a CSV?

```{r}
microbenchmark(
  base_R = write.csv(diamonds_big, "diamonds_big.csv"),
  tidyverse = write_csv(diamonds_tbl_big, "diamonds_big.csv"),
  data.table = fwrite(diamonds_dt_big, "diamonds_big.csv"),
    times = 1
)
```

How about we read this dataframe from the hard drive as a CSV?

```{r}
microbenchmark(
  base_R = read.csv("diamonds_big.csv"),
  tidyverse = read_csv("diamonds_big.csv"),
  data.table = fread("diamonds_big.csv"),
    times = 1
)
```

What about for creating new variables?

```{r}
microbenchmark(
  base_R = {diamonds_big$log_price = log(diamonds_big$price)},
  tidyverse = {diamonds_tbl_big %<>% mutate(log_price = log(price))},
  data.table = diamonds_dt_big[, log_price := log(price)],
    times = 100
)
```

About the same. How about grouping and summarizing? No easy one-liner in base R. So we just compare the two packages:


```{r}
microbenchmark(
  tidyverse = {diamonds_tbl_big %>% group_by(color) %>% summarize(avg_price = mean(price))},
  data.table = diamonds_dt_big[, .(avg_price = mean(price), by = color)],
    times = 10
)
```

How about sorting?

```{r}
microbenchmark(
  base_R = diamonds_big[order(diamonds_big$price), ],
  tidyverse = {diamonds_tbl_big %>% arrange(price)},
  data.table = diamonds_dt_big[order(price)],
    times = 10
)
```
How about filtering?

```{r}
microbenchmark(
  base_R = diamonds_big[diamonds_big$price < 1000, ],
  tidyverse = {diamonds_tbl_big %>% filter(price < 1000)},
  data.table = diamonds_dt_big[price < 1000],
    times = 10
)
```

Let's do this again but first "key" the price column which is what you would do if you are doing lots of searches.

```{r}
setkey(diamonds_dt_big, price)

microbenchmark(
  base_R = diamonds_big[diamonds_big$price < 1000, ],
  tidyverse = {diamonds_tbl_big %>% filter(price < 1000)},
  data.table = diamonds_dt_big[price < 1000],
    times = 30
)
```

We still have to learn how to reshape tables and join multiple tables together. We will do that later.

